{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX0ibVfSFDHu"
      },
      "source": [
        "# Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmWsi3taFGsy"
      },
      "outputs": [],
      "source": [
        "! pip3 install tiktoken\n",
        "! pip3 install torch # torchvision torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZqRuSVIE50F"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXc3b-PWE5aG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(123)\n",
        "torch.set_printoptions(sci_mode=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1_vBWq9DRD8"
      },
      "source": [
        "# Loading Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDxqxMyZC6Wx"
      },
      "outputs": [],
      "source": [
        "file_path = \"the-verdict.txt\"\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "text_data = \"\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        text_data = response.read().decode('utf-8')\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text_data)\n",
        "else:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLSms0UpKE6G"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHpsDxhrKBS8",
        "outputId": "c336b0ac-3317-4ec3-fbe8-90766127ce8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
            "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
          ]
        }
      ],
      "source": [
        "# Sanity Check\n",
        "\n",
        "# First 100 characters\n",
        "print(text_data[:99])\n",
        "# Last 100 characters\n",
        "print(text_data[-99:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi0pHudMDbJ9"
      },
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPERI4s2EG4b"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8X6ALGJEsFf",
        "outputId": "9edc0992-3729-4b61-debb-c21b83864f1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Characters : 20479\n",
            "Number of Tokens     : 5145\n"
          ]
        }
      ],
      "source": [
        "# Sanity Check\n",
        "\n",
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(\"Number of Characters :\", total_characters)\n",
        "print(\"Number of Tokens     :\", total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOmA-TbPGAsx"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X79sbT0AF-FU"
      },
      "outputs": [],
      "source": [
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3K4kbxyHs8-",
        "outputId": "1b55ec4f-8b39-468f-eaa3-dcee9efd7b49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n",
            "[tensor([[2885, 1464, 1807, 3619]]), tensor([[1464, 1807, 3619,  402]])]\n",
            "[tensor([[1464, 1807, 3619,  402]]), tensor([[1807, 3619,  402,  271]])]\n",
            "[tensor([[1807, 3619,  402,  271]]), tensor([[ 3619,   402,   271, 10899]])]\n"
          ]
        }
      ],
      "source": [
        "# Sanity Check\n",
        "\n",
        "dataloader = create_dataloader(txt=text_data, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "\n",
        "batch_01 = next(data_iter)\n",
        "batch_02 = next(data_iter)\n",
        "batch_03 = next(data_iter)\n",
        "batch_04 = next(data_iter)\n",
        "batch_05 = next(data_iter)\n",
        "\n",
        "print(batch_01)\n",
        "print(batch_02)\n",
        "print(batch_03)\n",
        "print(batch_04)\n",
        "print(batch_05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBXlR596N5Uf",
        "outputId": "e9f9fdeb-4444-4568-c4f9-b5300f4c90ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs:\n",
            "tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            "tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader(txt=text_data, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print(\"Inputs:\")\n",
        "print(inputs)\n",
        "print(\"\\nTargets:\")\n",
        "print(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XM4swF9I6kV"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,   # Vocabulary size\n",
        "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
        "    \"emb_dim\": 768,        # Embedding dimension\n",
        "    \"n_heads\": 12,         # Number of attention heads\n",
        "    \"n_layers\": 12,        # Number of layers\n",
        "    \"drop_rate\": 0.1,      # Dropout rate\n",
        "    \"qkv_bias\": False      # Query-key-value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px0oURMNLUio"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.90  # Train/validation ratio\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "\n",
        "train_data = text_data[:split_idx]\n",
        "val_data   = text_data[split_idx:]\n",
        "\n",
        "train_loader = create_dataloader(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq6PKDsdL76w",
        "outputId": "6d998d3d-490e-44fe-9b45-91c3f3b7856f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "train_loader length : 9\n",
            "val_loader length   : 1\n",
            "Training tokens     : 4608\n",
            "Validation tokens   : 512\n",
            "All tokens          : 5120\n"
          ]
        }
      ],
      "source": [
        "# Sanity check\n",
        "\n",
        "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the training loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"increase the `training_ratio`\")\n",
        "\n",
        "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the validation loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"decrease the `training_ratio`\")\n",
        "\n",
        "train_tokens = 0\n",
        "for input_batch, target_batch in train_loader:\n",
        "    train_tokens += input_batch.numel()\n",
        "\n",
        "val_tokens = 0\n",
        "for input_batch, target_batch in val_loader:\n",
        "    val_tokens += input_batch.numel()\n",
        "\n",
        "\n",
        "# Print\n",
        "\n",
        "print(\"Train loader:\")\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\ntrain_loader length :\", len(train_loader))\n",
        "print(\"val_loader length   :\", len(val_loader))\n",
        "print(\"Training tokens     :\", train_tokens)\n",
        "print(\"Validation tokens   :\", val_tokens)\n",
        "print(\"All tokens          :\", train_tokens + val_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhuhD5gBP9sp"
      },
      "source": [
        "# Important Sub-Blocks of the Transformer Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LLo4oslONSk"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "\n",
        "        return self.scale * norm_x + self.shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPkPtIfjQKyr"
      },
      "outputs": [],
      "source": [
        "class GELU(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3obHiyIMQKPn"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), # Expansion\n",
        "            GELU(),                                        # Activation\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), # Contraction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQj7tJIrQOg6"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(\n",
        "                torch.ones(context_length, context_length),\n",
        "                diagonal=1\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQUzB8fUY0ys"
      },
      "source": [
        "# The Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG4B-rxwQUAy"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"]\n",
        "        )\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvQefer8ZDSm"
      },
      "source": [
        "# GPT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgWo_2K7QXlJ"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval();  # Disable dropout during inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlGIM9GepKyT"
      },
      "source": [
        "# Important Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Gx8zCPDZKUc"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNNC6qyQZSwf",
        "outputId": "46a6abf4-a1fc-4877-de30-8ec5c8f84a76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 10.98069847954644\n",
            "Validation loss: 10.94732666015625\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Note:\n",
        "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
        "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
        "# However, the resulting loss values may be slightly different.\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    device = torch.device(\"cuda\")\n",
        "#elif torch.backends.mps.is_available():\n",
        "#    device = torch.device(\"mps\")\n",
        "#else:\n",
        "#    device = torch.device(\"cpu\")\n",
        "#\n",
        "# print(f\"Using {device} device.\")\n",
        "\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ0rsMS4ZaC6"
      },
      "outputs": [],
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrQuxx-sZTfH"
      },
      "outputs": [],
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuGgL5sIZVkd"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkc0zQlSZgRA"
      },
      "outputs": [],
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DLV4066pHmI"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgaPbj9sZjBJ",
        "outputId": "c8e5c65d-5217-40c7-ec0e-516a08cee358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.783, Val loss 9.927\n",
            "Ep 1 (Step 000005): Train loss 7.985, Val loss 8.335\n",
            "Every effort moves you,,,,,,,,,,,,.                                     \n",
            "Ep 2 (Step 000010): Train loss 6.753, Val loss 7.048\n",
            "Ep 2 (Step 000015): Train loss 6.114, Val loss 6.573\n",
            "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,,,,, and,,,, and,, and,,,,, and,,,,,,\n",
            "Ep 3 (Step 000020): Train loss 5.525, Val loss 6.490\n",
            "Ep 3 (Step 000025): Train loss 5.324, Val loss 6.387\n",
            "Every effort moves you, and to the picture.                      \"I, and the of the of the's the honour, and, and I had been, and I\n",
            "Ep 4 (Step 000030): Train loss 4.761, Val loss 6.360\n",
            "Ep 4 (Step 000035): Train loss 4.461, Val loss 6.258\n",
            "Every effort moves you of the to the picture--as of the picture--as I had been \" it was his \" I was the     \"I was his I had been the his pictures--and it the picture and I had been the picture of\n",
            "Ep 5 (Step 000040): Train loss 3.833, Val loss 6.196\n",
            "Every effort moves you know the \"Oh, and he was not the fact by his last word.         \"I was.      \"Oh, I felt a little a little the    \n",
            "Ep 6 (Step 000045): Train loss 3.352, Val loss 6.139\n",
            "Ep 6 (Step 000050): Train loss 2.861, Val loss 6.112\n",
            "Every effort moves you know; and my dear, and he was not the fact with a little of the house of the fact of the fact, and.                       \n",
            "Ep 7 (Step 000055): Train loss 2.347, Val loss 6.138\n",
            "Ep 7 (Step 000060): Train loss 2.084, Val loss 6.179\n",
            "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I looked--as of the fact, and I felt him--his back his head to the donkey. \"Oh, and_--because he had always _\n",
            "Ep 8 (Step 000065): Train loss 1.521, Val loss 6.176\n",
            "Ep 8 (Step 000070): Train loss 1.272, Val loss 6.178\n",
            "Every effort moves you?\" \"I didn't bear the picture--I told me.  \"I looked up, and went on groping and Mrs. I was back the head to look up at the honour being _mine_--because he was when I\n",
            "Ep 9 (Step 000075): Train loss 1.000, Val loss 6.277\n",
            "Ep 9 (Step 000080): Train loss 0.718, Val loss 6.281\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 10 (Step 000085): Train loss 0.506, Val loss 6.325\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to the donkey again. I saw that, and down the room, when I\n",
            "Training completed in 22.76 minutes.\n"
          ]
        }
      ],
      "source": [
        "# Note:\n",
        "# Uncomment the following code to calculate the execution time\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Note:\n",
        "# Uncomment the following code to show the execution time\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmNu6sbHpPS_"
      },
      "source": [
        "# Plot the Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "gGlmyNWAn7SR",
        "outputId": "9f8c7455-d290-43a8-b25e-b6d546498b4c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FTpjvMdSpRkr",
        "outputId": "50ba03e1-7e63-442d-936f-1ba031daa481"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(\"cpu\")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2sBJICYsTdZ"
      },
      "source": [
        "# Decoding Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMsH5OUGyiJF"
      },
      "source": [
        "### Strategy 0 : **No Extra Strategy** (Greedy Decoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpAQfSqhpXZn",
        "outputId": "a88f9077-29e6-4f7e-f602-7c0dd7c62fc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort moves you?\"\n",
            "\n",
            "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=25,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCCV4VFD6dwX"
      },
      "source": [
        "### Strategy 1 : **Temperature Scaling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19U_jWsB6rht"
      },
      "source": [
        "**Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlzvap3Ypoyt"
      },
      "outputs": [],
      "source": [
        "vocab = {\n",
        "    \"closer\": 0,\n",
        "    \"every\": 1,\n",
        "    \"effort\": 2,\n",
        "    \"forward\": 3,\n",
        "    \"inches\": 4,\n",
        "    \"moves\": 5,\n",
        "    \"pizza\": 6,\n",
        "    \"toward\": 7,\n",
        "    \"you\": 8,\n",
        "}\n",
        "\n",
        "inverse_vocab = {v: k for k, v in vocab.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmSExt-bp5Zr",
        "outputId": "5c554951-918b-4581-c5bc-6e5676c49ded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.1546, 0.0750, 0.0429, 0.2421, 0.0869, 0.0454, 0.0430, 0.2203, 0.0898])\n",
            "tensor([0.1546, 0.0750, 0.0429, 0.2421, 0.0869, 0.0454, 0.0430, 0.2203, 0.0898])\n"
          ]
        }
      ],
      "source": [
        "next_token_logits = torch.tensor([4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79])\n",
        "\n",
        "next_token_logits1 = next_token_logits / 0.1\n",
        "next_token_logits2 = next_token_logits / 5\n",
        "\n",
        "probas = torch.softmax(next_token_logits1, dim=0)\n",
        "probas = torch.softmax(next_token_logits2, dim=0)\n",
        "\n",
        "print(probas)\n",
        "print(probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5LbBZqq71Uk",
        "outputId": "52d1ad26-6e8c-48b2-ca14-ffae71b92eb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "probas:\n",
            "tensor([    0.0609,     0.0016,     0.0001,     0.5721,     0.0034,     0.0001,\n",
            "            0.0001,     0.3576,     0.0040])\n",
            "\n",
            "next_token_id : 3\n",
            "next_token    : forward\n"
          ]
        }
      ],
      "source": [
        "# using: SoftMax --> ArgMax\n",
        "\n",
        "probas = torch.softmax(next_token_logits, dim=0)\n",
        "next_token_id = torch.argmax(probas).item()\n",
        "\n",
        "print(\"probas:\")\n",
        "print(probas)\n",
        "print(\"\\nnext_token_id :\", next_token_id)\n",
        "print(\"next_token    :\", inverse_vocab[int(next_token_id)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMGJkfOf8S6q",
        "outputId": "b22f262a-64c6-4ff3-ae5d-869306795da9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "next_token_id : 7\n",
            "next_token    : toward\n"
          ]
        }
      ],
      "source": [
        "# Let's use Multinational instead of ArgMax\n",
        "\n",
        "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
        "\n",
        "print(\"next_token_id :\", next_token_id)\n",
        "print(\"next_token    :\", inverse_vocab[int(next_token_id)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOUnshDW84Sk",
        "outputId": "d950e31f-5298-4630-f261-8017c614ce6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "59 \t--> closer\n",
            "0 \t--> every\n",
            "0 \t--> effort\n",
            "593 \t--> forward\n",
            "2 \t--> inches\n",
            "0 \t--> moves\n",
            "0 \t--> pizza\n",
            "346 \t--> toward\n"
          ]
        }
      ],
      "source": [
        "def print_sampled_tokens(probas):\n",
        "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
        "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
        "    for i, freq in enumerate(sampled_ids):\n",
        "        print(f\"{freq} \\t--> {inverse_vocab[i]}\")\n",
        "\n",
        "print_sampled_tokens(probas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViSDrfk1MQ0k"
      },
      "source": [
        "**Main Code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I48QLiiyQCsi"
      },
      "source": [
        "$$ Logits \\; \\longrightarrow \\; Logits \\; / \\; Temperature \\; \\longrightarrow \\; SoftMax \\; \\longrightarrow \\; Multinomial \\; Distribution $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um4BvG9Z9QYd"
      },
      "outputs": [],
      "source": [
        "# using: Logits/Temperature --> SoftMax --> Multinomial\n",
        "\n",
        "def nextToken(logits, temperature=1.0):\n",
        "    logits = logits / temperature\n",
        "    probas = torch.softmax(logits, dim=-1)\n",
        "    return torch.multinomial(probas, num_samples=1).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "wT2DhCLCKTPb",
        "outputId": "3178e7c2-8858-40c5-f797-73d244de6417"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Temperature values\n",
        "temperatures = [1, 0.1, 3]  # Original, higher confidence, and lower confidence\n",
        "\n",
        "# Calculate scaled probabilities\n",
        "scaled_probas = [nextToken(next_token_logits, T) for T in temperatures]\n",
        "\n",
        "# Plotting\n",
        "x = torch.arange(len(vocab))\n",
        "bar_width = 0.15\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 3))\n",
        "for i, T in enumerate(temperatures):\n",
        "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
        "\n",
        "ax.set_ylabel('Probability')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"temperature-plot.pdf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySYLSfaMMc3L"
      },
      "source": [
        "### Strategy 2 : **Top-K Sampling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6uZA2jlNoGy"
      },
      "source": [
        "**Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1zLwkLaKmu3",
        "outputId": "44062082-edef-486d-fc16-95c33a9e9fe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top logits    : tensor([6.7500, 6.2800, 4.5100])\n",
            "Top positions : tensor([3, 7, 0])\n",
            "\n",
            "new_logits:\n",
            "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n",
            "\n",
            "topk_probas\n",
            "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
          ]
        }
      ],
      "source": [
        "top_k = 3\n",
        "\n",
        "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
        "\n",
        "new_logits = torch.where(\n",
        "    condition=next_token_logits < top_logits[-1],\n",
        "    input=torch.tensor(float(\"-inf\")),\n",
        "    other=next_token_logits\n",
        ")\n",
        "\n",
        "topk_probas = torch.softmax(new_logits, dim=0)\n",
        "\n",
        "\n",
        "print(\"Top logits    :\", top_logits)\n",
        "print(\"Top positions :\", top_pos)\n",
        "print(\"\\nnew_logits:\")\n",
        "print(new_logits)\n",
        "print(\"\\ntopk_probas\")\n",
        "print(topk_probas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elaZSHZvSfcQ"
      },
      "source": [
        "**Main Code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_e5tKp3OrK4"
      },
      "source": [
        "$$ Logits \\; \\longrightarrow \\; Top\\,K \\; \\longrightarrow \\; Logits \\; / \\; Temperature \\; \\longrightarrow \\; SoftMax \\; \\longrightarrow \\; Multinomial \\; Distribution $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QleMHq9NNudB"
      },
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zgFSTy3SjPp",
        "outputId": "e37e4bc6-a537-4b24-fa6a-144f0d33e390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output text:\n",
            " Every effort moves youdragged him down his painting?\" The word smile the so inevitably the honour\n"
          ]
        }
      ],
      "source": [
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=15,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#\n",
        "---\n",
        "#### Need Larger Dataset and Need More Training for Better Results. \n",
        "#### But for now, This Note Good-Enough to Understand/Implement a Transformer Model from (kind of) Scratch.\n",
        "---\n",
        "#"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
